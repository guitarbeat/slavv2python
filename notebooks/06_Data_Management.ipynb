{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Data Management\n",
                "\n",
                "This notebook helps manage your comparison data, including listing runs, analyzing disk usage, cleaning up old checkpoints, and generating documentation artifacts."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "\n",
                "import sys\n",
                "from pathlib import Path\n",
                "import pandas as pd\n",
                "import ipywidgets as widgets\n",
                "from IPython.display import display, clear_output\n",
                "\n",
                "# Add project root to path\n",
                "project_root = Path('..').resolve()\n",
                "sys.path.insert(0, str(project_root))\n",
                "\n",
                "from slavv.dev.management import (\n",
                "    list_runs, \n",
                "    analyze_checkpoints, \n",
                "    cleanup_checkpoints, \n",
                "    generate_manifest,\n",
                "    format_size\n",
                ")\n",
                "\n",
                "comparisons_dir = project_root / 'comparisons'\n",
                "if not comparisons_dir.exists():\n",
                "    print(f\"Creating {comparisons_dir}\")\n",
                "    comparisons_dir.mkdir(exist_ok=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. List Comparison Runs\n",
                "\n",
                "View all previous runs and their status."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "runs = list_runs(comparisons_dir)\n",
                "\n",
                "if not runs:\n",
                "    print(\"No runs found.\")\n",
                "else:\n",
                "    # Create a nice DataFrame summary\n",
                "    df_data = []\n",
                "    for r in runs:\n",
                "        content = []\n",
                "        if r['has_matlab']: content.append('MATLAB')\n",
                "        if r['has_python']: content.append('Python')\n",
                "        \n",
                "        df_data.append({\n",
                "            'Run Name': r['name'],\n",
                "            'Size': format_size(r['size']),\n",
                "            'Content': ', '.join(content),\n",
                "            'Speedup': f\"{r.get('speedup', 0):.1f}x\" if r.get('speedup') else '-'\n",
                "        })\n",
                "    \n",
                "    display(pd.DataFrame(df_data))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Disk Usage Analysis & Cleanup\n",
                "\n",
                "Identify runs taking up space with intermediate checkpoint files (`.pkl`) and clean them up."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def refresh_analysis():\n",
                "    clear_output(wait=True)\n",
                "    print(\"Analyzing disk usage...\\n\")\n",
                "    \n",
                "    checkpoint_data = analyze_checkpoints(comparisons_dir)\n",
                "    total_saved = 0\n",
                "    \n",
                "    for data in checkpoint_data:\n",
                "        if data['pkl_count'] > 0:\n",
                "            print(f\"üìÇ {data['name']}\")\n",
                "            print(f\"   Total Size: {format_size(data['total_size'])}\")\n",
                "            print(f\"   Checkpoints: {data['pkl_count']} files ({format_size(data['pkl_size'])})\")\n",
                "            \n",
                "            btn = widgets.Button(description=f\"Clean {format_size(data['pkl_size'])}\", button_style='warning')\n",
                "            \n",
                "            def clean_callback(b, d=data):\n",
                "                freed = cleanup_checkpoints(d)\n",
                "                b.description = f\"Freed {format_size(freed)}\"\n",
                "                b.disabled = True\n",
                "                b.button_style = 'success'\n",
                "            \n",
                "            btn.on_click(clean_callback)\n",
                "            display(btn)\n",
                "            print(\"-\" * 40)\n",
                "            \n",
                "    if not any(d['pkl_count'] > 0 for d in checkpoint_data):\n",
                "        print(\"‚úÖ No checkpoints found. Disk is clean!\")\n",
                "\n",
                "refresh_btn = widgets.Button(description=\"Refresh Analysis\", icon='refresh')\n",
                "refresh_btn.on_click(lambda b: refresh_analysis())\n",
                "display(refresh_btn)\n",
                "\n",
                "refresh_analysis()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Documentation Generation\n",
                "\n",
                "Generate `MANIFEST.md` inventory files for your comparison runs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "runs = list_runs(comparisons_dir)\n",
                "generated_count = 0\n",
                "\n",
                "print(\"Generating manifests...\\n\")\n",
                "for r in runs:\n",
                "    try:\n",
                "        manifest_path = r['path'] / 'MANIFEST.md'\n",
                "        generate_manifest(r['path'], manifest_path)\n",
                "        print(f\"‚úÖ Generated for {r['name']}\")\n",
                "        generated_count += 1\n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Failed for {r['name']}: {e}\")\n",
                "\n",
                "print(f\"\\nDone! Generated {generated_count} manifests.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}